# Sonified Latent Data :microphone::abacus:	
## Experiments to sonify different latent distributions generated by encoders from multiple datasets.

This repository is made to assist my university thesis about sonifying the latent space. We couple a WaveNet + VAE similar to [[Chorowski et al., 2019]](https://arxiv.org/abs/1901.08810) with latent distrubitions generated from multiple different dataset-encoder couples. 

The goal is to discover if the sonified audio results contains meaningfull differences and features from the input datasets.
A lot of earlier experiments relevant to this project I tried using the WaveNet decoder can be found in my [denoising repository](https://github.com/WouterBesse/ConvDenoiser).

## How To Use

- First install all dependencies from requirements.txt
- Every model has its' own notebook containing training and inference instructions
- Models can be downloaded from here: (to be made)
- Datasets will are listed at the end of this readme

## Models
### Standard WaveNet VAE

This model is very similar to the one described by [Chorowski et al.](https://arxiv.org/abs/1901.08810) and follows the following model:
I decided to go with a normal VAE and not the quantized variant because it allows me to more easily interpolate and play with the latent space.

![WaveNet VAE Diagram](https://github.com/WouterBesse/Sonified-Latent-Data/blob/main/media/WaveNetVae.jpg?raw=true)

#### Training and model

My model is downloadable from 'n.b.t.', I trained it on the LJSpeech dataset. You can train your own model using `train.py` from the WaveNetVAE folder or by using the `WaveVaePlayground.ipynb` jupyter notebook.

example usage of CLI train.py: 

`python3 train.py -tp "./traindatasetfolder/" -vp "./validationdatasetfolder/" -ep 100`
| **Short Flag** | **Long Flag**       | **Description**                                                   |
|----------------|---------------------|-------------------------------------------------------------------|
| `-tp`          | `--train_path`      | Path of folder where training audio data is stored                |
| `-vp`          | `--validation_path` | Path of folder where validation audio data is stored              |
| `-ep`          | `--epochs`          | Amount of epochs to train                                         |
| `-ex`          | `--export_path`     | Path of folder to export model files to                           |
| `-bs`          | `--batch_size`      | Batch size                                                        |
| `-lr`          | `--learning_rate`   | Learning rate, I recommend 0.00001                                |
| `-kla`         | `--kl_anneal`       | How much the kl rate multiplier is increased after every log step |
| `-mkl`         | `--max_kl`          | What the maximum kl rate multiplier will be                       |
| `-lpe`         | `--logs_per_epoch`  | How often a tensorboard log is stored per epoch                   |
| `-d`           | `--device`          | What device to train on, e.g. `cuda:0`, `cpu`                     |
| `-mf`          | `--max_files`       | The maximum amount of files to use in the train dataset           |


### Tybalt WaveNet VAE

A alteration on the [Tybalt VAE model](https://github.com/greenelab/tybalt) by [Way et al.](https://www.biorxiv.org/content/10.1101/174474v2).
I gave it one extra linear layer to help reducing the data to a smaller latent space.

![Tybalt VAE Diagram](https://github.com/WouterBesse/Sonified-Latent-Data/blob/main/media/Tybalt.svg?raw=true)

#### Training and model

My model is downloadable from 'n.b.t.', it's trained on the TCGA dataset. You can train your own model using `train.py` from the WaveNetVAE folder or by using the `WaveVaePlayground.ipynb` jupyter notebook.
The acquisition and preprocessing scripts are available in the original [Tybalt GitHub](https://github.com/greenelab/tybalt).

example usage of CLI train.py: 

`python3 train.py -tp "./traindatasetfolder/" -vp "./validationdatasetfolder/" -ep 100`
| **Short Flag** | **Long Flag**       | **Description**                                                                       |
|----------------|---------------------|---------------------------------------------------------------------------------------|
| `-dp`          | `--data_path`       | Path of folder where training data is stored, validation split is made automatically. |
| `-ep`          | `--epochs`          | Amount of epochs to train                                                             |
| `-ex`          | `--export_path`     | Path of folder to export model files to                                               |
| `-bs`          | `--batch_size`      | Batch size                                                                            |
| `-lr`          | `--learning_rate`   | Learning rate, I recommend 0.00001                                                    |
| `-kla`         | `--kl_anneal`       | How much the kl rate multiplier is increased after every log step                     |
| `-mkl`         | `--max_kl`          | What the maximum kl rate multiplier will be                                           |
| `-lpe`         | `--logs_per_epoch`  | How often a tensorboard log is stored per epoch                                       |
| `-d`           | `--device`          | What device to train on, e.g. `cuda:0`, `cpu`                                         |
| `-mf`          | `--max_files`       | The maximum amount of files to use in the train dataset                               |

### Mocap WaveNet VAE


## Datasets

